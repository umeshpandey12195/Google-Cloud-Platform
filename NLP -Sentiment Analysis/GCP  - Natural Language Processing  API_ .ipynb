{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import All the Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from google.cloud import vision\n",
    "from google.cloud.vision import enums\n",
    "import cv2 as cv\n",
    "\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "\n",
    "import gzip\n",
    "\n",
    "from google.cloud.vision_v1.proto import image_annotator_pb2\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "import re\n",
    "from google.cloud import vision\n",
    "from google.cloud import storage\n",
    "from google.protobuf import json_format\n",
    "from google.cloud import storage\n",
    "\n",
    "from google.cloud import language_v1\n",
    "from google.cloud.language_v1 import enums\n",
    "\n",
    "import argparse\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "from google.cloud import language\n",
    "import numpy\n",
    "import six"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Google application Credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"my-project-12195-46fed19566f2.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Classify a ‘text string’ passed by the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step to follow**\n",
    "\n",
    "**1-Classifying Content in a String**\n",
    "\n",
    "**2-Create a function that will take text_content as an input**\n",
    "\n",
    "**3-text_content the text content to analyze. Must include at least 20 words.**\n",
    "\n",
    "**4-Use LanguageServiceClient() for text analysis operations such as sentiment analysis and entityrecognition.**\n",
    "\n",
    "**5-Create a dictionary of document and passed the tex_content ,type as plain text and language as english**\n",
    "\n",
    "**6-Use classifyText requests, which classifies content into categories along with a confidence score**\n",
    "\n",
    "**7-Print the category name and confidence score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_classify_text(text_content):\n",
    "    \n",
    "    client = language_v1.LanguageServiceClient()\n",
    "\n",
    "    # text_content = 'That actor on TV makes movies in Hollywood and also stars in a variety of popular new TV shows.'\n",
    "\n",
    "    # Available types: PLAIN_TEXT, HTML\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "\n",
    "    # Optional. If not specified, the language is automatically detected.\n",
    "    # For list of supported languages:\n",
    "    # https://cloud.google.com/natural-language/docs/languages\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text_content, \"type\": type_, \"language\": language}\n",
    "\n",
    "    response = client.classify_text(document)\n",
    "    # Loop through classified categories returned from the API\n",
    "    for category in response.categories:\n",
    "        # Get the name of the category representing the document.\n",
    "        # See the predefined taxonomy of categories:\n",
    "        # https://cloud.google.com/natural-language/docs/categories\n",
    "        print(u\"Category name: {}\".format(category.name))\n",
    "        # Get the confidence. Number representing how certain the classifier\n",
    "        # is that this category represents the provided text.\n",
    "        print(u\"Confidence: {}\".format(category.confidence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the function and pass the Text string "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below text give the description of Mobile phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category name: /Internet & Telecom/Mobile & Wireless\n",
      "Confidence: 0.6299999952316284\n",
      "Category name: /Internet & Telecom/Service Providers\n",
      "Confidence: 0.5199999809265137\n"
     ]
    }
   ],
   "source": [
    "text_content = \"A mobile phone is a portable device that can make and receive calls over a radio frequency link while the user is moving within a telephone service area. The radio frequency link establishes a connection to the switching systems of a mobile phone operator, which provides access to the public switched telephone network (PSTN).\"\n",
    "sample_classify_text(text_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**\n",
    "\n",
    "The Code correctly Predict the category as mobile and wireless  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Classify text  files in a local directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Function that will take text file as an input and predict category ,confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_classify_file(file_name): \n",
    "    file=open(file_name,'r')\n",
    "    text_content=file.read()\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "    language = \"en\"\n",
    "    document = {\"content\": text_content, \"type\": type_, \"language\": language}\n",
    "    response = client.classify_text(document)\n",
    "    for category in response.categories:\n",
    "        print(u\"Category name: {}\".format(category.name))\n",
    "        print(u\"Confidence: {}\".format(category.confidence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call the function and pass the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category name: /Internet & Telecom/Mobile & Wireless\n",
      "Confidence: 0.6299999952316284\n",
      "Category name: /Internet & Telecom/Service Providers\n",
      "Confidence: 0.5199999809265137\n"
     ]
    }
   ],
   "source": [
    "file_name='mobile.txt'\n",
    "sample_classify_file(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Classify .pdf and .txt files stored on google Storage Bucket "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function that will take url of the text/pdf stored in google storage and return  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_classify_uri(gcs_content_uri):\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "    type_ = enums.Document.Type.PLAIN_TEXT\n",
    "    language = \"en\"\n",
    "    document = {\"gcs_content_uri\": gcs_content_uri, \"type\": type_, \"language\": language}\n",
    "    response = client.classify_text(document)\n",
    "    for category in response.categories:\n",
    "        print(u\"Category name: {}\".format(category.name))\n",
    "        print(u\"Confidence: {}\".format(category.confidence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call the function and pass the link of text file stored in google storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category name: /Arts & Entertainment/Movies\n",
      "Confidence: 0.9599999785423279\n"
     ]
    }
   ],
   "source": [
    "gcs_content_uri = \"gs://training-12195/textanalysis.txt\"\n",
    "sample_classify_uri(gcs_content_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call the function and pass the link of PDF file stored in google storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category name: /Computers & Electronics/Software\n",
      "Confidence: 0.7099999785423279\n"
     ]
    }
   ],
   "source": [
    "gcs_content_uri=\"gs://training-12195/text.pdf\"\n",
    "sample_classify_uri(gcs_content_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-Index multiple text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index function in the tutorial script takes, as input, a directory containing multiple text files, and the path to a file where it stores the indexed output (the default file name is index.json). The index function reads the content of each text file in the input directory, and then passes the text files to the Cloud Natural Language API to be classified into content categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step**\n",
    "\n",
    "**1-Create a function index that will take folder where all the text file are stored and index_file by where our json file will be stored**\n",
    "\n",
    "**2-Iterate over each text file extract the content and call classify function to get the category and Confidence Score**\n",
    "\n",
    "**3-Classify function will return Category and Confidence score and we will save for every text in json format**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text, verbose=True):\n",
    "    \"\"\"Classify the input text into categories. \"\"\"\n",
    "\n",
    "    language_client = language.LanguageServiceClient()\n",
    "\n",
    "    document = language.types.Document(\n",
    "        content=text,\n",
    "        type=language.enums.Document.Type.PLAIN_TEXT)\n",
    "    response = language_client.classify_text(document)\n",
    "    categories = response.categories\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    for category in categories:\n",
    "        # Turn the categories into a dictionary of the form:\n",
    "        # {category.name: category.confidence}, so that they can\n",
    "        # be treated as a sparse vector.\n",
    "        result[category.name] = category.confidence\n",
    "\n",
    "    if verbose:\n",
    "        print(text)\n",
    "        for category in categories:\n",
    "            print(u'=' * 20)\n",
    "            print(u'{:<16}: {}'.format('category', category.name))\n",
    "            print(u'{:<16}: {}'.format('confidence', category.confidence))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index(path, index_file):\n",
    "    \"\"\"Classify each text file in a directory and write\n",
    "    the results to the index_file.\n",
    "    \"\"\"\n",
    "\n",
    "    result = {}\n",
    "    for filename in os.listdir(path):\n",
    "        file_path = os.path.join(path, filename)\n",
    "\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with io.open(file_path, 'r') as f:\n",
    "                text = f.read()\n",
    "                categories = classify(text, verbose=False)\n",
    "\n",
    "                result[filename] = categories\n",
    "        except Exception:\n",
    "            print('Failed to process {}'.format(file_path))\n",
    "\n",
    "    with io.open(index_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(result, ensure_ascii=False))\n",
    "\n",
    "    print('Texts indexed in file: {}'.format(index_file))\n",
    "    print()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts indexed in file: index\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Android.txt': {'/Computers & Electronics': 0.800000011920929,\n",
       "  '/Internet & Telecom/Mobile & Wireless/Mobile Apps & Add-Ons': 0.6499999761581421},\n",
       " 'Cloud Computing.txt': {'/Computers & Electronics/Networking': 0.6800000071525574,\n",
       "  '/Internet & Telecom/Web Services': 0.5400000214576721},\n",
       " 'harry potter.txt': {\"/Books & Literature/Children's Literature\": 0.9200000166893005,\n",
       "  \"/People & Society/Kids & Teens/Children's Interests\": 0.8500000238418579,\n",
       "  '/People & Society/Subcultures & Niche Interests': 0.5600000023841858},\n",
       " 'matildata.txt': {\"/Books & Literature/Children's Literature\": 0.9900000095367432,\n",
       "  \"/People & Society/Kids & Teens/Children's Interests\": 0.9700000286102295},\n",
       " 'Wireless.txt': {'/Internet & Telecom': 0.7099999785423279}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'Text_file'\n",
    "index_file = 'index'\n",
    "index(path, index_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Find files in a directory that are most similar to a query label passed by the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query with category labels\n",
    "Once the index file (default file name = index.json) has been created, we can make queries to the index to retrieve some of the filenames and their confidence scores.\n",
    "\n",
    "One way to do this is to use a category label as the query, which the tutorial accomplishes with the query_category function. The implementation of the helper functions, such as similarity, can be found in the classify_text_tutorial.py file. In your applications the similarity scoring and ranking should be carefully designed around specific use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP**\n",
    "\n",
    "**1-Create a function query_category that will take Jason file we have created,category_string want to search as an input**\n",
    "\n",
    "**2-Open the jason file and call the similarity function that will take the query category and the category prsent in json**\n",
    "\n",
    "**3-Inside the similarity function we will call the split function that will calculate the similarities inside the subcategory**\n",
    "\n",
    "\n",
    "    \"\"\"The category labels are of the form \"/a/b/c\" up to three levels,\n",
    "    for example \"/Computers & Electronics/Software\", and these labels\n",
    "    are used as keys in the categories dictionary, whose values are\n",
    "    confidence scores.\n",
    "    The split_labels function splits the keys into individual levels\n",
    "    while duplicating the confidence score, which allows a natural\n",
    "    boost in how we calculate similarity when more levels are in common.\n",
    "    Example:\n",
    "    If we have\n",
    "    x = {\"/a/b/c\": 0.5}\n",
    "    y = {\"/a/b\": 0.5}\n",
    "    z = {\"/a\": 0.5}\n",
    "    Then x and y are considered more similar than y and z.\n",
    "    \"\"\"\n",
    "    \n",
    "**4-The split function will be called twice for both the category and than we will normalize the output for both**\n",
    "\n",
    "**5- Finally will calculate cosine similarity and confidence score**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create query_category Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_category(index_file, category_string, n_top=3):\n",
    "    \"\"\"Find the indexed files that are the most similar to\n",
    "    the query label.\n",
    "\n",
    "    The list of all available labels:\n",
    "    https://cloud.google.com/natural-language/docs/categories\n",
    "    \"\"\"\n",
    "\n",
    "    with io.open(index_file, 'r') as f:\n",
    "        index = json.load(f)\n",
    "\n",
    "    # Make the category_string into a dictionary so that it is\n",
    "    # of the same format as what we get by calling classify.\n",
    "    query_categories = {category_string: 1.0}\n",
    "    print(index)\n",
    "    similarities = []\n",
    "    for filename, categories in six.iteritems(index):\n",
    "        similarities.append(\n",
    "            (filename, similarity(query_categories, categories)))\n",
    "\n",
    "    similarities = sorted(similarities, key=lambda p: p[1], reverse=True)\n",
    "\n",
    "    print('=' * 20)\n",
    "    print('Query: {}\\n'.format(category_string))\n",
    "    print('\\nMost similar {} indexed texts:'.format(n_top))\n",
    "    for filename, sim in similarities[:n_top]:\n",
    "        print('\\tFilename: {}'.format(filename))\n",
    "        print('\\tSimilarity: {}'.format(sim))\n",
    "        print('\\n')\n",
    "\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create similarities function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def similarity(categories1, categories2):\n",
    "    \"\"\"Cosine similarity of the categories treated as sparse vectors.\"\"\"\n",
    "    categories1 = split_labels(categories1)\n",
    "    categories2 = split_labels(categories2)\n",
    "\n",
    "    norm1 = numpy.linalg.norm(list(categories1.values()))\n",
    "    norm2 = numpy.linalg.norm(list(categories2.values()))\n",
    "\n",
    "    # Return the smallest possible similarity if either categories is empty.\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Compute the cosine similarity.\n",
    "    dot = 0.0\n",
    "    for label, confidence in six.iteritems(categories1):\n",
    "        dot += confidence * categories2.get(label, 0.0)\n",
    "\n",
    "    return dot / (norm1 * norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create split_labels function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_labels(categories):\n",
    "    \n",
    "    _categories = {}\n",
    "    for name, confidence in six.iteritems(categories):\n",
    "        labels = [label for label in name.split('/') if label]\n",
    "        for label in labels:\n",
    "            _categories[label] = confidence\n",
    "\n",
    "    return _categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call query_category function pass the jason file and category string we want to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Android.txt': {'/Computers & Electronics': 0.800000011920929, '/Internet & Telecom/Mobile & Wireless/Mobile Apps & Add-Ons': 0.6499999761581421}, 'Cloud Computing.txt': {'/Computers & Electronics/Networking': 0.6800000071525574, '/Internet & Telecom/Web Services': 0.5400000214576721}, 'harry potter.txt': {\"/Books & Literature/Children's Literature\": 0.9200000166893005, \"/People & Society/Kids & Teens/Children's Interests\": 0.8500000238418579, '/People & Society/Subcultures & Niche Interests': 0.5600000023841858}, 'matildata.txt': {\"/Books & Literature/Children's Literature\": 0.9900000095367432, \"/People & Society/Kids & Teens/Children's Interests\": 0.9700000286102295}, 'Wireless.txt': {'/Internet & Telecom': 0.7099999785423279}}\n",
      "====================\n",
      "Query: /Internet & Telecom/Mobile & Wireless\n",
      "\n",
      "\n",
      "Most similar 3 indexed texts:\n",
      "\tFilename: Wireless.txt\n",
      "\tSimilarity: 0.7071067811865476\n",
      "\n",
      "\n",
      "\tFilename: Android.txt\n",
      "\tSimilarity: 0.6655735790452499\n",
      "\n",
      "\n",
      "\tFilename: Cloud Computing.txt\n",
      "\tSimilarity: 0.31094107737232335\n",
      "\n",
      "\n",
      "[('Wireless.txt', 0.7071067811865476), ('Android.txt', 0.6655735790452499), ('Cloud Computing.txt', 0.31094107737232335), ('harry potter.txt', 0.0), ('matildata.txt', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "index_file='index'\n",
    "category_string = \"/Internet & Telecom/Mobile & Wireless\"\n",
    "Similarities=query_category(index_file, category_string)\n",
    "print(Similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**\n",
    "\n",
    "We have five text file present in the folder out of which wirelesse.txt is having the highest similarity with the query category\n",
    "followed by android.txt followed by cloud computing.txt.\n",
    "\n",
    "The remaining two file is having no similarity with the query category\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
